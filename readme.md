## Запуск
### Основная модель
Запустите `llama-cpp-linux/install.sh` чтобы скачать и собрать llama.cpp. В папке с собраной llama.cpp перейдите по пути `build/bin`, в ней храняться исполняемые файлы llama.cpp. Для наших целей необходимо запустить `llama-server -m <model> --port <port>`, где `<model>` - путь к `.gguf` файлу LLM, а `<port>` - порт который сервер начнёт прослушивать. (Рекомендуется использовать [Qwen3-4B-Instruct](https://huggingface.co/marcelone/Qwen3-4B-Instruct-2507-gguf)).
> Порт захардкожен на 10101
### Эмбеддер
Также нужно запустить эмбеддер на 10100 порту с добавленным флагом `--embedding`

### Запуск вебсервера
Подтяните зависимости для python указанные в `pyweb/requirements.txt`. Запустите `app.py`.
## Зависимости для сборки моделя на C++
1. `libssl`
2. `curl`
3. `meson`
